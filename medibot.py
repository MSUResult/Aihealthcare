import os
import streamlit as st

# ‚úÖ Disable CUDA to avoid `libcublas.so` error (CPU only)
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

# ‚úÖ Correct import for embeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEndpoint

# ‚úÖ Path to your FAISS vectorstore
DB_FAISS_PATH = "vectorstore/db_faiss"

# ‚úÖ Cache the loaded vectorstore
@st.cache_resource
def get_vectorstore():
    try:
        embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
        db = FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)
        return db
    except Exception as e:
        st.error(f"Failed to load vectorstore: {e}")
        return None

# ‚úÖ Custom prompt for better control over LLM responses
def set_custom_prompt(custom_prompt_template):
    return PromptTemplate(template=custom_prompt_template, input_variables=["context", "question"])

# ‚úÖ Load HuggingFace model from endpoint
def load_llm(huggingface_repo_id, HF_TOKEN):
    if not HF_TOKEN:
        st.error("‚ùå Hugging Face Token (HF_TOKEN) not set in environment.")
        raise ValueError("HF_TOKEN is not available")
    
    return HuggingFaceEndpoint(
        repo_id=huggingface_repo_id,
        temperature=0.5,
        model_kwargs={
            "token": HF_TOKEN,
            "max_length": "512"
        }
    )

# ‚úÖ Main Streamlit app
def main():
    st.title("üß† Ask Chatbot (MediBot)")

    # ‚úÖ Session state for chat history
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        st.chat_message(message['role']).markdown(message['content'])

    prompt = st.chat_input("Pass your prompt here")

    if prompt:
        st.chat_message('user').markdown(prompt)
        st.session_state.messages.append({'role': 'user', 'content': prompt})

        # ‚úÖ Custom Prompt Template
        CUSTOM_PROMPT_TEMPLATE = """
        Use the pieces of information provided in the context to answer user's question.
        If you don't know the answer, just say that you don't know. Don't try to make up an answer.
        Don't provide anything outside the given context.

        Context: {context}
        Question: {question}

        Start the answer directly. No small talk please.
        """

        # ‚úÖ HuggingFace repo and token
        HUGGINGFACE_REPO_ID = "mistralai/Mistral-7B-Instruct-v0.3"
        HF_TOKEN = os.environ.get("HF_TOKEN")

        try:
            vectorstore = get_vectorstore()
            if vectorstore is None:
                return

            # ‚úÖ QA Chain setup
            qa_chain = RetrievalQA.from_chain_type(
                llm=load_llm(HUGGINGFACE_REPO_ID, HF_TOKEN),
                chain_type="stuff",
                retriever=vectorstore.as_retriever(search_kwargs={'k': 3}),
                return_source_documents=True,
                chain_type_kwargs={'prompt': set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)}
            )

            response = qa_chain.invoke({'query': prompt})
            result = response["result"]
            source_documents = response["source_documents"]

            result_to_show = result + "\n\nüìö **Source Docs:**\n" + str(source_documents)
            st.chat_message('assistant').markdown(result_to_show)
            st.session_state.messages.append({'role': 'assistant', 'content': result_to_show})

        except Exception as e:
            st.error(f"üö® Error occurred: {str(e)}")

# ‚úÖ Run the app
if __name__ == "__main__":
    main()
